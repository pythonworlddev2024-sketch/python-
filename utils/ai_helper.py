"""
Assistant IA pour l'analyse de donn√©es
Utilise analyse locale intelligente + Google Gemini (gratuit) optionnel + Hugging Face API fallback
"""
import os
import re
import pandas as pd
import numpy as np
import requests
from difflib import SequenceMatcher


def fuzzy_contains(text, keywords, threshold=0.75):
    """Cherche si un mot similaire √† keywords existe dans text (tol√®re les fautes)"""
    words = text.lower().split()
    for keyword in keywords:
        for word in words:
            similarity = SequenceMatcher(None, keyword.lower(), word).ratio()
            if similarity >= threshold:
                return True
    return False


class UnrecognizedQuestion(Exception):
    """Exception lev√©e quand DataAnalyzerAI ne reconna√Æt pas la question"""
    pass


class DataAnalyzerAI:
    """IA conversationnelle pour analyser les donn√©es et r√©pondre √† TOUT"""
    
    def __init__(self, df):
        self.df = df
        self.analysis_cache = {}
        self._analyze_data()
    
    def _analyze_data(self):
        """Analyse compl√®te du DataFrame"""
        self.analysis_cache = {
            'shape': (len(self.df), len(self.df.columns)),
            'columns': self.df.columns.tolist(),
            'dtypes': {col: str(self.df[col].dtype) for col in self.df.columns},
            'missing': self.df.isnull().sum().to_dict(),
            'duplicates': len(self.df[self.df.duplicated()]),
            'numeric_cols': self.df.select_dtypes(include=[np.number]).columns.tolist(),
            'categorical_cols': self.df.select_dtypes(include=['object']).columns.tolist(),
            'stats': {}
        }
        
        # Statistiques pour colonnes num√©riques
        for col in self.analysis_cache['numeric_cols']:
            self.analysis_cache['stats'][col] = {
                'mean': float(self.df[col].mean()),
                'median': float(self.df[col].median()),
                'min': float(self.df[col].min()),
                'max': float(self.df[col].max()),
                'std': float(self.df[col].std()),
                'q25': float(self.df[col].quantile(0.25)),
                'q75': float(self.df[col].quantile(0.75)),
                'trend': self._calculate_trend(self.df[col])
            }
    
    def _calculate_trend(self, series):
        """Calcule la tendance (augmente/diminue/stable)"""
        if len(series) < 2:
            return "stable"
        first_half = series.iloc[:len(series)//2].mean()
        second_half = series.iloc[len(series)//2:].mean()
        if second_half > first_half * 1.05:
            return "augmente"
        elif second_half < first_half * 0.95:
            return "diminue"
        return "stable"
    
    def answer_question(self, question: str) -> str:
        """R√©pond UNIQUEMENT √† la question pos√©e, sans suggestions suppl√©mentaires"""
        q_lower = question.lower().strip()
        
        # === SALUTATIONS ===
        greetings = ['salut', 'hello', 'hi', 'bonjour', 'bonsoir', '√ßa va', 'how are you', 'quoi de neuf']
        if fuzzy_contains(q_lower, greetings, threshold=0.65):
            return f"üëã Salut ! Vous avez {self.analysis_cache['shape'][0]} lignes et {self.analysis_cache['shape'][1]} colonnes."
        
        # === REMERCIEMENTS ===
        thanks = ['merci', 'thank you', 'thanks', 'gracias']
        if fuzzy_contains(q_lower, thanks, threshold=0.65):
            return "üôè De rien !"
        
        # === PR√âDICTIONS/TENDANCES (EN PRIORIT√â) ===
        if fuzzy_contains(q_lower, ['pr√©dict', 'prediction', 'predir', 'predit', 'predict', 'forecast', 'futur', 'future', 'tendance', 'trend', 'extrapoler'], threshold=0.65):
            # Si la question contient des param√®tres (=, nombres), c'est une pr√©diction avanc√©e ‚Üí Gemini
            if '=' in q_lower or any(char.isdigit() for char in q_lower):
                raise UnrecognizedQuestion(question)
            
            numeric_cols = self.analysis_cache['numeric_cols']
            if not numeric_cols:
                return "‚ùå Pas de colonnes num√©riques pour pr√©dictions"
            
            # Chercher colonne sp√©cifique
            for col in numeric_cols:
                if col.lower() in q_lower:
                    trend = self.analysis_cache['stats'][col]['trend']
                    if trend == "augmente":
                        return f"üìà {col}: En augmentation (continue √† monter)"
                    elif trend == "diminue":
                        return f"üìâ {col}: En diminution (continue √† descendre)"
                    else:
                        return f"‚û°Ô∏è {col}: Stable (pas de changement majeur)"
            
            # Sinon toutes les colonnes
            trends = []
            for col in numeric_cols[:3]:
                trend = self.analysis_cache['stats'][col]['trend']
                emoji = "üìà" if trend == "augmente" else "üìâ" if trend == "diminue" else "‚û°Ô∏è"
                trends.append(f"{emoji} {col}: {trend}")
            return "üîÆ Tendances:\n" + "\n".join(trends)
        
        # === LIGNES ===
        if fuzzy_contains(q_lower, ['ligne', 'row', 'enregistrement', 'observation', 'record'], threshold=0.65):
            if fuzzy_contains(q_lower, ['combien', 'nombre', 'how', 'total', 'quoi'], threshold=0.65):
                return f"üìä {self.analysis_cache['shape'][0]} lignes"
        
        # === COLONNES ===
        if fuzzy_contains(q_lower, ['colonne', 'column', 'variable', 'feature', 'champ'], threshold=0.65):
            if fuzzy_contains(q_lower, ['combien', 'nombre', 'how', 'quoi'], threshold=0.65):
                numeric = len(self.analysis_cache['numeric_cols'])
                categorical = len(self.analysis_cache['categorical_cols'])
                return f"üìã {self.analysis_cache['shape'][1]} colonnes ({numeric} num√©riques, {categorical} cat√©goriques)"
        
        # === DONN√âES MANQUANTES ===
        if fuzzy_contains(q_lower, ['manquant', 'missing', 'null', 'nan', 'vide', 'empty'], threshold=0.65):
            total_missing = sum(self.analysis_cache['missing'].values())
            if total_missing == 0:
                return "‚úÖ 0 valeurs manquantes"
            else:
                top_missing = sorted(self.analysis_cache['missing'].items(), key=lambda x: x[1], reverse=True)[0]
                return f"‚ö†Ô∏è {total_missing} valeurs manquantes (colonne la plus touch√©e: {top_missing[0]} avec {top_missing[1]})"
        
        # === DOUBLONS ===
        if fuzzy_contains(q_lower, ['doublon', 'duplicate', 'doublons', 'duplique', 'r√©p√©t√©', 'unique', 'identique'], threshold=0.65):
            dups = self.analysis_cache['duplicates']
            if dups == 0:
                return "‚úÖ 0 doublons"
            else:
                return f"‚ö†Ô∏è {dups} doublons d√©tect√©s"
        
        # === MOYENNE - EXTRAIRE LA COLONNE SI MENTIONN√âE ===
        if fuzzy_contains(q_lower, ['moyenne', 'mean', 'moyen', 'average', 'avg', 'moyene', 'moyennes', 'moy'], threshold=0.65):
            numeric_cols = self.analysis_cache['numeric_cols']
            if not numeric_cols:
                return "‚ùå Aucune colonne num√©rique"
            
            # Chercher si colonne sp√©cifique mentionn√©e
            for col in numeric_cols:
                if col.lower() in q_lower:
                    mean = self.analysis_cache['stats'][col]['mean']
                    return f"üìä Moyenne de {col}: {mean:.2f}"
            
            # Sinon retourner toutes
            stats = []
            for col in numeric_cols:
                mean = self.analysis_cache['stats'][col]['mean']
                stats.append(f"{col}: {mean:.2f}")
            return "üìä Moyennes:\n" + "\n".join(stats)
        
        # === MINIMUM ===
        if fuzzy_contains(q_lower, ['minimum', 'min'], threshold=0.65) and not fuzzy_contains(q_lower, ['max'], threshold=0.65):
            numeric_cols = self.analysis_cache['numeric_cols']
            if not numeric_cols:
                return "‚ùå Aucune colonne num√©rique"
            
            # Chercher colonne sp√©cifique
            for col in numeric_cols:
                if col.lower() in q_lower:
                    min_val = self.analysis_cache['stats'][col]['min']
                    return f"üìä Min de {col}: {min_val:.2f}"
            
            stats = []
            for col in numeric_cols:
                min_val = self.analysis_cache['stats'][col]['min']
                stats.append(f"{col}: {min_val:.2f}")
            return "üìä Minimales:\n" + "\n".join(stats)
        
        # === MAXIMUM ===
        if fuzzy_contains(q_lower, ['max', 'maximum'], threshold=0.65) and not fuzzy_contains(q_lower, ['min', 'minimum'], threshold=0.65):
            numeric_cols = self.analysis_cache['numeric_cols']
            if not numeric_cols:
                return "‚ùå Aucune colonne num√©rique"
            
            # Chercher colonne sp√©cifique
            for col in numeric_cols:
                if col.lower() in q_lower:
                    max_val = self.analysis_cache['stats'][col]['max']
                    return f"üìä Max de {col}: {max_val:.2f}"
            
            stats = []
            for col in numeric_cols:
                max_val = self.analysis_cache['stats'][col]['max']
                stats.append(f"{col}: {max_val:.2f}")
            return "üìä Maximales:\n" + "\n".join(stats)
        
        # === MIN ET MAX ENSEMBLE ===
        if fuzzy_contains(q_lower, ['min', 'minimum'], threshold=0.65) and fuzzy_contains(q_lower, ['max', 'maximum'], threshold=0.65):
            numeric_cols = self.analysis_cache['numeric_cols']
            if not numeric_cols:
                return "‚ùå Aucune colonne num√©rique"
            
            # Chercher colonne sp√©cifique
            for col in numeric_cols:
                if col.lower() in q_lower:
                    min_val = self.analysis_cache['stats'][col]['min']
                    max_val = self.analysis_cache['stats'][col]['max']
                    return f"üìä {col}: Min={min_val:.2f}, Max={max_val:.2f}"
            
            stats = []
            for col in numeric_cols:
                min_val = self.analysis_cache['stats'][col]['min']
                max_val = self.analysis_cache['stats'][col]['max']
                stats.append(f"{col}: Min={min_val:.2f}, Max={max_val:.2f}")
            return "üìä Min/Max:\n" + "\n".join(stats)
        
        # === √âCART-TYPE / STD ===
        if fuzzy_contains(q_lower, ['√©cart', 'std', 'standard', 'deviation', 'variabilite', 'variation', 'dispersion', 'ecart'], threshold=0.65):
            numeric_cols = self.analysis_cache['numeric_cols']
            if not numeric_cols:
                return "‚ùå Aucune colonne num√©rique"
            
            # Chercher colonne sp√©cifique
            for col in numeric_cols:
                if col.lower() in q_lower:
                    std = self.analysis_cache['stats'][col]['std']
                    return f"üìä √âcart-type de {col}: {std:.2f}"
            
            stats = []
            for col in numeric_cols:
                std = self.analysis_cache['stats'][col]['std']
                stats.append(f"{col}: {std:.2f}")
            return "üìä √âcarts-types:\n" + "\n".join(stats)
        
        # === M√âDIANE ===
        if fuzzy_contains(q_lower, ['m√©diane', 'median', 'mediane', 'centre'], threshold=0.65):
            numeric_cols = self.analysis_cache['numeric_cols']
            if not numeric_cols:
                return "‚ùå Aucune colonne num√©rique"
            
            for col in numeric_cols:
                if col.lower() in q_lower:
                    median = self.analysis_cache['stats'][col]['median']
                    return f"üìä M√©diane de {col}: {median:.2f}"
            
            stats = []
            for col in numeric_cols:
                median = self.analysis_cache['stats'][col]['median']
                stats.append(f"{col}: {median:.2f}")
            return "üìä M√©dianes:\n" + "\n".join(stats)
        
        # === R√âSUM√â ===
        if fuzzy_contains(q_lower, ['r√©sum√©', 'resume', 'summary', 'apercu', 'overview', 'total', 'recap'], threshold=0.65):
            numeric_count = len(self.analysis_cache['numeric_cols'])
            cat_count = len(self.analysis_cache['categorical_cols'])
            total_missing = sum(self.analysis_cache['missing'].values())
            return f"""üìä R√©sum√©:
- {self.analysis_cache['shape'][0]} lignes √ó {self.analysis_cache['shape'][1]} colonnes
- {numeric_count} num√©riques, {cat_count} cat√©goriques
- {self.analysis_cache['duplicates']} doublons, {total_missing} manquantes"""
        
        
        # === NETTOYAGE ===
        if fuzzy_contains(q_lower, ['nettoyer', 'clean', 'probleme', 'probl√®me', 'issue', 'nettoie'], threshold=0.65):
            recs = []
            if self.analysis_cache['duplicates'] > 0:
                recs.append(f"Supprimer {self.analysis_cache['duplicates']} doublons")
            if sum(self.analysis_cache['missing'].values()) > 0:
                recs.append(f"Traiter {sum(self.analysis_cache['missing'].values())} manquantes")
            
            if not recs:
                return "‚úÖ Donn√©es tr√®s propres, aucun nettoyage n√©cessaire"
            return "üßπ √Ä faire:\n" + "\n".join(recs)
        
        # === VISUALISATION ===
        if fuzzy_contains(q_lower, ['visuali', 'graphique', 'plot', 'chart', 'graph', 'image', 'affiche'], threshold=0.65):
            return "üìà Utilisez l'onglet Visualisation pour cr√©er: scatter, line, bar, histogram, box, violin"
        
        # === FALLBACK : Lever exception pour essayer Gemini ===
        raise UnrecognizedQuestion(question)


def get_ai_response(question: str, context: str = None, df=None) -> str:
    """
    Google Gemini r√©pond √† TOUTES les questions avec analyse avanc√©e
    Inclut: statistiques descriptives, corr√©lations, tendances, distributions
    """
    
    api_key = ""
    try:
        import streamlit as st
        # Try session state first (user-provided key)
        api_key = st.session_state.get("google_api_key", "")
        # If not in session, try secrets
        if not api_key:
            candidates = [
                "GOOGLE_API_KEY", "google_api_key", "googleApiKey",
                "google.api_key", "googleapikey", "api_key"
            ]
            for key in candidates:
                if key in st.secrets:
                    api_key = st.secrets[key]
                    break
            # Try nested dict
            if not api_key and "google" in st.secrets:
                google_sect = st.secrets["google"]
                if isinstance(google_sect, dict):
                    api_key = google_sect.get("api_key") or google_sect.get("GOOGLE_API_KEY") or ""
    except Exception:
        pass

    if not api_key:
        api_key = os.getenv("GOOGLE_API_KEY", "")

    # üîë CL√â API PAR D√âFAUT - REMPLACEZ PAR VOTRE CL√â GEMINI
    if not api_key:
        api_key = "AIzaSyDtsa9wm5wbanlZ-UGTAv6Zs73vARraIYk"  # ‚Üê CL√â GEMINI CONFIGUR√âE

    api_key = (api_key or "").strip()
    
    if not api_key:
        # Fallback: utiliser Hugging Face API (GRATUIT, pas de cl√© n√©cessaire)
        hf_response = try_huggingface_api(question, df)
        if hf_response:
            return hf_response
        # FALLBACK FINAL: R√©ponses intelligentes bas√©es sur stats locales (TOUJOURS disponible)
        if df is not None:
            return generate_local_insight(question, df)
        # Si pas de donn√©es du tout
        return "‚ùå Veuillez charger des donn√©es pour utiliser l'IA."
    
    try:
        import google.generativeai as genai
        genai.configure(api_key=api_key)
        model = genai.GenerativeModel('gemini-2.5-flash')
        
        # Pr√©parer le contexte AVANC√â des donn√©es
        context_text = ""
        if df is not None and len(df) > 0:
            context_text = _build_advanced_context(df)
        
        prompt = f"""Tu dois r√©pondre EXTR√äMEMENT BRI√àVEMENT en une ou deux phrases maximum.
Pas d'explications, pas de d√©tails, pas de conseils.
Juste la r√©ponse directe √† la question.

{context_text}

Question: {question}

R√©ponse ultra-courte (1-2 phrases MAX):"""
        
        response = model.generate_content(prompt)
        if response and response.text:
            return response.text.strip()
        else:
            return "‚ùå Pas de r√©ponse de Gemini"
            
    except Exception as e:
        print(f"‚ùå Erreur Google: {str(e)}")
        # Fallback √† Hugging Face en cas d'erreur Google aussi
        hf_response = try_huggingface_api(question, df)
        if hf_response:
            return hf_response
        return f"‚ùå Service indisponible: {str(e)}"


def _build_advanced_context(df) -> str:
    """
    Construit un contexte enrichi avec corr√©lations, distributions, tendances
    """
    lines = []
    lines.append(f"CONTEXTE DONN√âES:")
    lines.append(f"- Nombre total de lignes: {len(df)}")
    lines.append(f"- Nombre de colonnes: {len(df.columns)}")
    
    # S√©parer num√©rique et cat√©gorique
    numeric_cols = df.select_dtypes(include=['int64', 'float64']).columns.tolist()
    categorical_cols = df.select_dtypes(include=['object']).columns.tolist()
    
    # === STATISTIQUES DESCRIPTIVES AVANC√âES ===
    if numeric_cols:
        lines.append(f"\nSTATISTIQUES DESCRIPTIVES ({len(numeric_cols)} colonnes num√©riques):")
        for col in numeric_cols:
            try:
                col_data = df[col].dropna()
                if len(col_data) > 0:
                    mean_val = col_data.mean()
                    std_val = col_data.std()
                    min_val = col_data.min()
                    max_val = col_data.max()
                    median_val = col_data.median()
                    q1 = col_data.quantile(0.25)
                    q3 = col_data.quantile(0.75)
                    
                    # Calculer la variabilit√© (coefficient de variation)
                    cv = (std_val / mean_val * 100) if mean_val != 0 else 0
                    
                    # Tendance (premi√®re moiti√© vs deuxi√®me moiti√©)
                    first_half = col_data.iloc[:len(col_data)//2].mean()
                    second_half = col_data.iloc[len(col_data)//2:].mean()
                    trend = "‚ÜóÔ∏è Augmente" if second_half > first_half * 1.05 else ("‚ÜòÔ∏è Diminue" if second_half < first_half * 0.95 else "‚Üí Stable")
                    
                    lines.append(f"\n  üìä {col}:")
                    lines.append(f"     ‚Ä¢ √âtendue: {min_val:.2f} √† {max_val:.2f} (variation = {max_val - min_val:.2f})")
                    lines.append(f"     ‚Ä¢ Moyenne: {mean_val:.2f} ¬± {std_val:.2f} (√©cart-type)")
                    lines.append(f"     ‚Ä¢ M√©diane: {median_val:.2f}")
                    lines.append(f"     ‚Ä¢ Quartiles: Q1={q1:.2f}, Q3={q3:.2f} (IQR={q3-q1:.2f})")
                    lines.append(f"     ‚Ä¢ Variabilit√©: {cv:.1f}% (coef. variation)")
                    lines.append(f"     ‚Ä¢ Tendance: {trend}")
            except Exception as e:
                lines.append(f"  ‚ùå Erreur pour {col}: {e}")
    
    # === CORR√âLATIONS ENTRE VARIABLES ===
    if len(numeric_cols) > 1:
        try:
            corr_matrix = df[numeric_cols].corr()
            lines.append(f"\nCORR√âLATIONS ENTRE VARIABLES:")
            
            # Trouvez les paires de corr√©lations importantes
            important_corrs = []
            for i in range(len(numeric_cols)):
                for j in range(i+1, len(numeric_cols)):
                    corr_val = corr_matrix.iloc[i, j]
                    if abs(corr_val) > 0.3:  # Seuil de corr√©lation significative
                        important_corrs.append((numeric_cols[i], numeric_cols[j], corr_val))
            
            # Trier par valeur absolue
            important_corrs.sort(key=lambda x: abs(x[2]), reverse=True)
            
            if important_corrs:
                for var1, var2, corr_val in important_corrs[:10]:  # Top 10
                    strength = "Tr√®s forte" if abs(corr_val) > 0.8 else ("Forte" if abs(corr_val) > 0.6 else ("Mod√©r√©e" if abs(corr_val) > 0.4 else "Faible"))
                    direction = "positive" if corr_val > 0 else "n√©gative"
                    lines.append(f"  ‚Ä¢ {var1} ‚Üî {var2}: {corr_val:.3f} ({strength} {direction})")
            else:
                lines.append(f"  ‚Ä¢ Pas de corr√©lations notables (|r| > 0.3)")
        except Exception as e:
            lines.append(f"\nCORR√âLATIONS: Erreur - {e}")
    
    # === QUALIT√â DES DONN√âES ===
    missing_total = df.isnull().sum().sum()
    duplicates = df.duplicated().sum()
    
    if missing_total > 0 or duplicates > 0:
        lines.append(f"\nQUALIT√â DES DONN√âES:")
        if missing_total > 0:
            pct_missing = (missing_total / (len(df) * len(df.columns))) * 100
            lines.append(f"  ‚Ä¢ Valeurs manquantes: {int(missing_total)} ({pct_missing:.1f}%)")
        if duplicates > 0:
            lines.append(f"  ‚Ä¢ Doublons: {int(duplicates)} lignes")
    
    # === COLONNES CAT√âGORIQUE ===
    if categorical_cols:
        lines.append(f"\nCOLONNES CAT√âGORIQUE:")
        for col in categorical_cols[:3]:  # Max 3 pour ne pas surcharger
            try:
                unique_count = df[col].nunique()
                lines.append(f"  ‚Ä¢ {col}: {unique_count} cat√©gories uniques")
            except Exception:
                pass
    
    return "\n".join(lines)


def try_huggingface_api(question: str, df=None) -> str:
    """
    Utiliser Hugging Face Inference API - version optimis√©e pour stabilit√©
    """
    try:
        # Construire un contexte MINIMALISTE mais efficace
        context_text = ""
        if df is not None and len(df) > 0:
            numeric_cols = df.select_dtypes(include=['int64', 'float64']).columns.tolist()
            
            # Stats simples et rapides
            context_text = f"Rows: {len(df)}, Cols: {len(df.columns)}\n"
            
            # Juste les stats essentielles des colonnes num√©riques (max 3)
            for col in numeric_cols[:3]:
                try:
                    col_data = df[col].dropna()
                    if len(col_data) > 1:
                        mean_val = col_data.mean()
                        min_val = col_data.min()
                        max_val = col_data.max()
                        
                        # Tendance simple
                        first_half = col_data.iloc[:len(col_data)//2].mean()
                        second_half = col_data.iloc[len(col_data)//2:].mean()
                        trend = "UP" if second_half > first_half * 1.05 else ("DOWN" if second_half < first_half * 0.95 else "STABLE")
                        
                        context_text += f"{col}: mean={mean_val:.1f}, min={min_val:.1f}, max={max_val:.1f}, trend={trend}\n"
                except:
                    pass
        
        # API Hugging Face - mod√®le tr√®s l√©ger et rapide
        url = "https://api-inference.huggingface.co/models/gpt2"
        
        # Prompt ultra-simplifi√©
        prompt = f"Analyze data and answer concisely (1 sentence):\n{context_text}\nQuestion: {question}\nAnswer:"
        
        headers = {"Authorization": "Bearer hf_placeholder"}
        
        payload = {
            "inputs": prompt,
            "parameters": {
                "max_new_tokens": 60,
                "temperature": 0.5,
            }
        }
        
        # Essayer avec timeout court d'abord
        for timeout in [15, 25]:
            try:
                response = requests.post(url, json=payload, headers=headers, timeout=timeout)
                
                if response.status_code == 200:
                    result = response.json()
                    if isinstance(result, list) and len(result) > 0:
                        answer = result[0].get("generated_text", "")
                        # Extract answer after "Answer:"
                        if "Answer:" in answer:
                            answer = answer.split("Answer:")[-1]
                        answer = answer.strip()
                        if answer and len(answer) > 3:
                            return answer
                    return None
                elif response.status_code == 503:
                    # Model loading, retry with longer timeout
                    continue
                else:
                    return None
            except requests.Timeout:
                if timeout == 15:
                    continue  # Try with longer timeout
                else:
                    return None
        
        return None
    
    except Exception as e:
        print(f"HF error: {e}")
        return None


def generate_local_insight(question: str, df) -> str:
    """
    G√©n√©rer des insights locaux ROBUSTES avec RECOMMANDATIONS ACTIONNABLES.
    R√©pond √† des questions sp√©cifiques sur les VRAIES donn√©es du DataFrame.
    Inclut conseils pratiques pour l'analyse.
    """
    if df is None or df.empty:
        return "‚ùå Aucune donn√©e disponible. Veuillez d'abord charger vos donn√©es dans l'onglet Upload."
    
    question_lower = question.lower().strip()
    lignes, colonnes = df.shape
    
    try:
        # === QUESTIONS SUR LES DONN√âES G√âN√âRALES ===
        if any(word in question_lower for word in ["combien", "nombre", "how many", "total", "count"]):
            if any(word in question_lower for word in ["ligne", "row", "observation", "record"]):
                return f"üìä Vos donn√©es contiennent **{lignes} lignes**. üí° Conseil: Si c'est peu de donn√©es, consid√©rez collecter plus d'√©chantillons pour des analyses plus robustes."
            if any(word in question_lower for word in ["colonne", "column", "variable", "feature"]):
                return f"üìã Vos donn√©es contiennent **{colonnes} colonnes**. üí° Conseil: Utilisez l'onglet Analyse pour voir les types de chaque colonne."
            return f"üìä **{lignes}** lignes √ó **{colonnes}** colonnes. üí° Conseil: C'est un dataset de taille { 'petit' if lignes < 1000 else 'moyen' if lignes < 10000 else 'grand' }."
        
        # === QUESTIONS SUR LES COLONNES ===
        if any(word in question_lower for word in ["colonne", "column", "variable", "feature", "champ"]):
            col_names = ", ".join(df.columns[:5])
            if len(df.columns) > 5:
                col_names += f", ... ({len(df.columns) - 5} autres)"
            numeric_count = len(df.select_dtypes(include=['number']).columns)
            cat_count = len(df.select_dtypes(include=['object']).columns)
            return f"üìã Colonnes: {col_names}\nüí° **{numeric_count} num√©riques**, **{cat_count} cat√©goriques**. Conseil: Les colonnes num√©riques sont id√©ales pour les graphiques et pr√©dictions."
        
        # === QUESTIONS SUR LES VALEURS MANQUANTES ===
        if any(word in question_lower for word in ["manquant", "null", "missing", "vide", "empty", "nan"]):
            nulls = df.isnull().sum()
            null_total = nulls.sum()
            if null_total == 0:
                return "‚úÖ **Aucune valeur manquante** - vos donn√©es sont compl√®tes ! üí° Conseil: Excellente qualit√©, vous pouvez proc√©der directement √† l'analyse."
            null_pct = (null_total / (lignes * colonnes) * 100)
            top_nulls = nulls[nulls > 0].nlargest(3)
            top_text = ", ".join([f"**{col}**: {count} ({count/lignes*100:.1f}%)" for col, count in top_nulls.items()])
            advice = "üí° Conseil: Utilisez l'onglet Nettoyage ‚Üí 'Traiter valeurs manquantes' pour les remplacer par moyenne/m√©diane ou supprimer les lignes."
            return f"‚ö†Ô∏è **{null_total}** valeurs manquantes ({null_pct:.1f}%). Top colonnes: {top_text}. {advice}"
        
        # === QUESTIONS SUR LES DOUBLONS ===
        if any(word in question_lower for word in ["duplic", "duplicate", "doublon", "r√©p√©t√©", "identique"]):
            duplicates = df.duplicated().sum()
            if duplicates == 0:
                return "‚úÖ **Aucun doublon** d√©tect√© - donn√©es uniques ! üí° Conseil: Bonne qualit√©, pas besoin de nettoyage pour les doublons."
            dup_pct = (duplicates / lignes * 100)
            advice = "üí° Conseil: Allez dans Nettoyage ‚Üí 'Supprimer doublons' pour nettoyer automatiquement."
            return f"‚ö†Ô∏è **{duplicates} lignes dupliqu√©es** ({dup_pct:.1f}%). {advice}"
        
        # === QUESTIONS SUR LES TENDANCES ===
        if any(word in question_lower for word in ["tendance", "trend", "√©volution", "progression", "changement", "variation"]):
            numeric_cols = df.select_dtypes(include=['number']).columns
            if len(numeric_cols) == 0:
                return "‚ùå Pas de colonnes num√©riques pour analyser les tendances. üí° Conseil: Convertissez d'abord les colonnes textuelles en num√©riques si possible."
            
            insights = []
            for col in numeric_cols[:3]:
                try:
                    col_data = df[col].dropna()
                    if len(col_data) > 1:
                        first_half = col_data.iloc[:len(col_data)//2].mean()
                        second_half = col_data.iloc[len(col_data)//2:].mean()
                        if first_half != 0:
                            change_pct = (second_half - first_half) / first_half * 100
                            if change_pct > 5:
                                trend = f"üìà **{col}**: +{change_pct:.1f}% (augmentation)"
                            elif change_pct < -5:
                                trend = f"üìâ **{col}**: {change_pct:.1f}% (diminution)"
                            else:
                                trend = f"‚û°Ô∏è **{col}**: {change_pct:+.1f}% (stable)"
                            insights.append(trend)
                except:
                    pass
            
            if insights:
                result = "üìä Tendances d√©tect√©es:\n" + "\n".join(insights)
                result += "\nüí° Conseil: Cr√©ez un graphique lin√©aire dans Visualisation pour voir l'√©volution temporelle."
                return result
            return "‚û°Ô∏è Aucune tendance significative d√©tect√©e. üí° Conseil: V√©rifiez si vos donn√©es sont tri√©es chronologiquement."
        
        # === QUESTIONS SUR LES STATISTIQUES ===
        if any(word in question_lower for word in ["moyen", "moyenne", "mean", "average", "stats", "statistique", "r√©sum√©", "summary"]):
            numeric_cols = df.select_dtypes(include=['number']).columns
            if len(numeric_cols) == 0:
                return "‚ùå Pas de colonnes num√©riques. üí° Conseil: Utilisez l'onglet Analyse pour voir les statistiques des colonnes textuelles."
            
            stats_lines = []
            for col in numeric_cols[:5]:
                try:
                    col_data = df[col].dropna()
                    if len(col_data) > 0:
                        mean_val = col_data.mean()
                        std_val = col_data.std()
                        min_val = col_data.min()
                        max_val = col_data.max()
                        stats_lines.append(f"**{col}**: Œº={mean_val:.2f} ¬± {std_val:.2f}, [{min_val:.2f}, {max_val:.2f}]")
                except:
                    pass
            
            if stats_lines:
                result = "üìä Statistiques principales:\n" + "\n".join(stats_lines)
                result += "\nüí° Conseil: Pour une analyse compl√®te, allez dans l'onglet Analyse ‚Üí voir tous les d√©tails par colonne."
                return result
            return "‚ùå Erreur lors du calcul. üí° Conseil: V√©rifiez que vos colonnes num√©riques contiennent des nombres valides."
        
        # === QUESTIONS SUR LA DISTRIBUTION ===
        if any(word in question_lower for word in ["distrib", "distribution", "r√©partition", "spread", "dispersion"]):
            numeric_cols = df.select_dtypes(include=['number']).columns
            if len(numeric_cols) == 0:
                return "‚ùå Pas de colonnes num√©riques. üí° Conseil: Cr√©ez un histogramme dans Visualisation pour voir la distribution."
            
            insights = []
            for col in numeric_cols[:3]:
                try:
                    col_data = df[col].dropna()
                    if len(col_data) > 1:
                        skew = col_data.skew()
                        if abs(skew) > 1:
                            skew_type = "tr√®s asym√©trique" if abs(skew) > 2 else "asym√©trique"
                            direction = "√† droite" if skew > 0 else "√† gauche"
                            insights.append(f"üìä **{col}**: {skew_type} {direction} (asym√©trie={skew:.2f})")
                        else:
                            insights.append(f"üìä **{col}**: distribution sym√©trique")
                except:
                    pass
            
            if insights:
                result = "\n".join(insights)
                result += "\nüí° Conseil: Utilisez un box plot dans Visualisation pour d√©tecter les outliers."
                return result
            return "‚ùå Erreur lors de l'analyse. üí° Conseil: Nettoyez d'abord les valeurs aberrantes."
        
        # === QUESTIONS SUR LES CORR√âLATIONS ===
        if any(word in question_lower for word in ["corr", "relation", "correlation", "lien", "association", "d√©pend"]):
            numeric_cols = df.select_dtypes(include=['number']).columns
            if len(numeric_cols) < 2:
                return "‚ùå Besoin d'au moins 2 colonnes num√©riques. üí° Conseil: Ajoutez plus de variables num√©riques ou utilisez l'encodage pour les cat√©goriques."
            
            try:
                corr_matrix = df[numeric_cols].corr()
                corr_pairs = []
                
                for i in range(len(corr_matrix.columns)):
                    for j in range(i+1, len(corr_matrix.columns)):
                        corr_val = corr_matrix.iloc[i, j]
                        if abs(corr_val) > 0.3:  # Corr√©lation significative
                            strength = "tr√®s forte" if abs(corr_val) > 0.7 else "forte" if abs(corr_val) > 0.5 else "mod√©r√©e"
                            direction = "positive" if corr_val > 0 else "n√©gative"
                            corr_pairs.append(f"üîó **{corr_matrix.columns[i]}** ‚Üî **{corr_matrix.columns[j]}**: {corr_val:.3f} ({strength} {direction})")
                
                if corr_pairs:
                    result = "Corr√©lations d√©tect√©es:\n" + "\n".join(corr_pairs[:5])
                    result += "\nüí° Conseil: Cr√©ez un scatter plot dans Visualisation pour visualiser ces relations."
                    return result
                return "‚úÖ Aucune corr√©lation significative (|r| > 0.3). üí° Conseil: Les variables sont ind√©pendantes - int√©ressant pour la mod√©lisation !"
            except:
                return "‚ùå Erreur lors du calcul. üí° Conseil: V√©rifiez que vos donn√©es sont num√©riques et nettoy√©es."
        
        # === QUESTIONS SUR LES OUTLIERS ===
        if any(word in question_lower for word in ["outlier", "aberrant", "extr√™me", "anomalie", "valeur extr√™me"]):
            numeric_cols = df.select_dtypes(include=['number']).columns
            if len(numeric_cols) == 0:
                return "‚ùå Pas de colonnes num√©riques. üí° Conseil: Les outliers ne peuvent √™tre d√©tect√©s que sur des donn√©es num√©riques."
            
            total_outliers = 0
            outlier_info = []
            for col in numeric_cols:
                try:
                    col_data = df[col].dropna()
                    if len(col_data) > 0:
                        Q1 = col_data.quantile(0.25)
                        Q3 = col_data.quantile(0.75)
                        IQR = Q3 - Q1
                        lower_bound = Q1 - 1.5 * IQR
                        upper_bound = Q3 + 1.5 * IQR
                        outliers = ((col_data < lower_bound) | (col_data > upper_bound)).sum()
                        total_outliers += outliers
                        if outliers > 0:
                            outlier_info.append(f"‚ö†Ô∏è **{col}**: {outliers} outliers")
                except:
                    pass
            
            if total_outliers > 0:
                result = f"üìä **Nombre total d'outliers**: {total_outliers}\n"
                if outlier_info:
                    result += "D√©tail par colonne:\n" + "\n".join(outlier_info[:5])
                result += f"\nüí° Conseil: {total_outliers} valeurs extr√™mes d√©tect√©es. Utilisez Nettoyage ‚Üí 'Traiter outliers'."
                return result
            return "‚úÖ **Aucun outlier** d√©tect√© dans vos donn√©es num√©riques. üí° Conseil: Bonne qualit√© pour l'analyse statistique."
        
        # === QUESTIONS SUR LES CONSEILS D'AM√âLIORATION ===
        if any(word in question_lower for word in ["conseil", "advice", "conseils", "am√©liorer", "ameliorer", "suggestion", "recommandation", "comment", "aide", "help", "mieux", "better", "optimiser", "optimize"]):
            # Analyser les probl√®mes potentiels
            problems = []
            suggestions = []
            
            # V√©rifier les valeurs manquantes
            null_total = df.isnull().sum().sum()
            if null_total > 0:
                null_pct = (null_total / (lignes * colonnes) * 100)
                problems.append(f"‚ö†Ô∏è {null_total} valeurs manquantes ({null_pct:.1f}%)")
                suggestions.append("‚Ä¢ Remplacer par moyenne/m√©diane ou supprimer les lignes avec Nettoyage ‚Üí 'Traiter manquantes'")
            
            # V√©rifier les doublons
            duplicates = df.duplicated().sum()
            if duplicates > 0:
                dup_pct = (duplicates / lignes * 100)
                problems.append(f"‚ö†Ô∏è {duplicates} doublons ({dup_pct:.1f}%)")
                suggestions.append("‚Ä¢ Supprimer automatiquement avec Nettoyage ‚Üí 'Supprimer doublons'")
            
            # V√©rifier les outliers
            numeric_cols = df.select_dtypes(include=['number']).columns
            total_outliers = 0
            for col in numeric_cols:
                try:
                    col_data = df[col].dropna()
                    if len(col_data) > 0:
                        Q1 = col_data.quantile(0.25)
                        Q3 = col_data.quantile(0.75)
                        IQR = Q3 - Q1
                        lower_bound = Q1 - 1.5 * IQR
                        upper_bound = Q3 + 1.5 * IQR
                        outliers = ((col_data < lower_bound) | (col_data > upper_bound)).sum()
                        total_outliers += outliers
                except:
                    pass
            
            if total_outliers > 0:
                problems.append(f"‚ö†Ô∏è {total_outliers} outliers d√©tect√©s")
                suggestions.append("‚Ä¢ Traiter les valeurs extr√™mes avec Nettoyage ‚Üí 'Traiter outliers'")
            
            # V√©rifier les types de donn√©es
            cat_cols = df.select_dtypes(include=['object']).columns
            if len(cat_cols) > 0 and len(numeric_cols) == 0:
                problems.append("‚ö†Ô∏è Aucune colonne num√©rique")
                suggestions.append("‚Ä¢ Encoder les colonnes textuelles pour les analyses num√©riques")
            
            # Si pas de probl√®mes majeurs
            if not problems:
                suggestions = [
                    "‚Ä¢ Cr√©er des visualisations avec l'onglet Visualisation",
                    "‚Ä¢ Lancer une pr√©diction avec l'onglet Pr√©diction",
                    "‚Ä¢ Analyser les corr√©lations entre variables",
                    "‚Ä¢ Exporter vos r√©sultats nettoy√©s"
                ]
            
            result = "üí° **Conseils pour am√©liorer votre analyse :**\n\n"
            if problems:
                result += "**Probl√®mes identifi√©s :**\n" + "\n".join(problems) + "\n\n"
            result += "**Suggestions d'am√©lioration :**\n" + "\n".join(suggestions)
            return result
        
        # === QUESTIONS SUR LES LIGNES APR√àS NETTOYAGE ===
        if any(word in question_lower for word in ["rester", "restent", "resteront", "apr√®s", "after", "nettoyage", "cleaning", "suppression", "remove"]):
            # Estimer le nombre de lignes apr√®s nettoyage
            original_lines = lignes
            
            # Lignes supprim√©es pour doublons
            duplicates = df.duplicated().sum()
            lines_after_dedup = original_lines - duplicates
            
            # Lignes supprim√©es pour valeurs manquantes (estimation)
            null_pct = (df.isnull().sum().sum() / (lignes * colonnes) * 100)
            # Supposons qu'on supprime les lignes avec >50% de valeurs manquantes
            rows_with_many_nulls = (df.isnull().sum(axis=1) / colonnes > 0.5).sum()
            lines_after_nulls = lines_after_dedup - rows_with_many_nulls
            
            # Lignes supprim√©es pour outliers (estimation prudente)
            numeric_cols = df.select_dtypes(include=['number']).columns
            estimated_outlier_rows = 0
            for col in numeric_cols:
                try:
                    col_data = df[col].dropna()
                    if len(col_data) > 0:
                        Q1 = col_data.quantile(0.25)
                        Q3 = col_data.quantile(0.75)
                        IQR = Q3 - Q1
                        lower_bound = Q1 - 1.5 * IQR
                        upper_bound = Q3 + 1.5 * IQR
                        outlier_rows = ((col_data < lower_bound) | (col_data > upper_bound)).sum()
                        estimated_outlier_rows += outlier_rows * 0.1  # Estimation prudente
                except:
                    pass
            
            final_lines = max(1, int(lines_after_nulls - estimated_outlier_rows))
            
            result = f"üìä **Estimation des lignes apr√®s nettoyage :**\n\n"
            result += f"‚Ä¢ **Lignes originales**: {original_lines}\n"
            result += f"‚Ä¢ **Apr√®s suppression doublons**: {lines_after_dedup} (-{duplicates})\n"
            result += f"‚Ä¢ **Apr√®s traitement manquantes**: ~{lines_after_nulls} (-{rows_with_many_nulls})\n"
            result += f"‚Ä¢ **Apr√®s traitement outliers**: ~{final_lines}\n\n"
            result += f"üí° **{final_lines} lignes** devraient rester apr√®s un nettoyage complet."
            result += f"\nüí° Conseil: Utilisez l'onglet Nettoyage pour appliquer ces transformations automatiquement."
            
            return result
        
        # === QUESTIONS SUR LA VISUALISATION ===
        if any(word in question_lower for word in ["visuali", "graphique", "plot", "chart", "diagramme", "courbe"]):
            return "üìà **Onglet Visualisation** : cr√©ez scatter, line, bar, histogram, box, violin plots.\nüí° Conseil: Commencez par un histogramme pour voir les distributions, puis scatter pour les corr√©lations."
        
        # === QUESTIONS SUR LES PR√âDICTIONS ===
        if any(word in question_lower for word in ["pr√©dict", "mod√®le", "predict", "r√©gression", "classification", "machine learning", "ml"]):
            return "ü§ñ **Onglet Pr√©diction** : Random Forest pour regression/classification.\nüí° Conseil: Assurez-vous d'avoir nettoy√© les donn√©es et s√©lectionn√© une cible pertinente avant l'entra√Ænement."
        
        # === QUESTIONS SUR LE NETTOYAGE ===
        if any(word in question_lower for word in ["qualit√©", "quality", "nettoyer", "clean", "probl√®me", "issue", "am√©liorer"]):
            null_pct = (df.isnull().sum().sum() / (lignes * colonnes) * 100)
            duplicates = df.duplicated().sum()
            quality_score = 100 - null_pct - (duplicates / lignes * 100)
            
            result = f"‚ú® **Qualit√© des donn√©es**: {quality_score:.1f}/100\n"
            result += f"- {null_pct:.1f}% valeurs manquantes\n"
            result += f"- {duplicates} doublons\n"
            
            if quality_score > 80:
                result += "üí° **Excellente qualit√©** ! Vous pouvez proc√©der √† l'analyse."
            elif quality_score > 60:
                result += "üí° **Bonne qualit√©** avec quelques am√©liorations possibles."
            else:
                result += "üí° **Qualit√© √† am√©liorer** : utilisez l'onglet Nettoyage."
            
            return result
        
        # === QUESTIONS SUR LES TYPES DE DONN√âES ===
        if any(word in question_lower for word in ["type", "dtype", "format", "nature"]):
            dtypes = df.dtypes.value_counts()
            dtype_text = ", ".join([f"{count} colonnes {dtype}" for dtype, count in dtypes.items()])
            return f"üìù **Types de donn√©es**: {dtype_text}\nüí° Conseil: Les colonnes 'object' sont textuelles - encodez-les pour les pr√©dictions num√©riques."
        
        # === QUESTIONS G√âN√âRALES SUR LES DONN√âES ===
        if any(word in question_lower for word in ["quoi", "what", "tell", "dis", "info", "informations", "donne", "r√©sum√©", "summary", "aper√ßu", "overview"]):
            numeric_cols = df.select_dtypes(include=['number']).columns
            categorical_cols = df.select_dtypes(include=['object']).columns
            null_pct = (df.isnull().sum().sum() / (lignes * colonnes) * 100)
            
            summary = f"üìä **Aper√ßu de vos donn√©es:**\n"
            summary += f"- **{lignes}** lignes √ó **{colonnes}** colonnes\n"
            summary += f"- **{len(numeric_cols)}** colonnes num√©riques, **{len(categorical_cols)}** colonnes textuelles\n"
            summary += f"- **{null_pct:.1f}%** valeurs manquantes\n"
            summary += f"- **{df.duplicated().sum()}** doublons\n"
            summary += f"\nüí° **Prochaines √©tapes recommand√©es:**\n"
            summary += f"1. Nettoyez les donn√©es (si n√©cessaire)\n"
            summary += f"2. Explorez avec Visualisation\n"
            summary += f"3. Lancez une pr√©diction"
            return summary
        
        # === R√âPONSE PAR D√âFAUT INTELLIGENTE ===
        return f"üí° **{lignes}** lignes √ó **{colonnes}** colonnes charg√©es.\n\n‚ùì **Questions que je peux r√©pondre:**\n- Tendances et √©volutions üìà\n- Statistiques et moyennes üìä\n- Corr√©lations entre variables üîó\n- Qualit√© des donn√©es ‚ú®\n- D√©tection d'outliers ‚ö†Ô∏è\n\nüí° **Essayez:** 'Quelle est la tendance ?', 'Y a-t-il des corr√©lations ?', 'Qualit√© des donn√©es ?'"
    
    except Exception as e:
        return f"‚ùå Erreur lors de l'analyse: {str(e)[:50]}\nüí° Conseil: V√©rifiez que vos donn√©es sont au bon format et rechargez la page si n√©cessaire."


def generate_smart_response(question: str, context: str = None, df=None) -> str:
    """G√©n√©rer une r√©ponse intelligente avec les vraies donn√©es"""
    
    question_lower = question.lower()
    # Si un DataFrame est fourni, extraire les vraies valeurs
    if df is not None:
        try:
            lignes = int(len(df))
            colonnes = int(df.shape[1])
            doublons = int(df.duplicated().sum())
            missing_total = int(df.isnull().sum().sum())
            missing_by_col = (df.isnull().sum() / len(df) * 100).sort_values(ascending=False)
            numeric_cols = df.select_dtypes(include=['number']).columns.tolist()
        except Exception:
            lignes = extract_number(context, "lignes")
            colonnes = extract_number(context, "colonnes")
            doublons = extract_number(context, "doublons")
            missing_total = None
            missing_by_col = None
            numeric_cols = []
    else:
        # Extraire les chiffres du contexte
        context_lower = context.lower() if context else ""
        lignes = extract_number(context, "lignes")
        colonnes = extract_number(context, "colonnes")
        doublons = extract_number(context, "doublons")
        missing_total = None
        missing_by_col = None
        numeric_cols = []
    
    # Questions sur le nombre de lignes
    if "nombre" in question_lower and "ligne" in question_lower or ("combien" in question_lower and "ligne" in question_lower):
        return f"üìä {lignes} lignes dans votre dataset."

    if ("combien" in question_lower and "colonne" in question_lower) or ("nombre" in question_lower and "colonne" in question_lower):
        return f"üìã {colonnes} colonnes dans votre dataset."
    
    # Questions sur les doublons
    if "doublon" in question_lower or "duplicate" in question_lower:
        try:
            dcount = int(doublons)
        except Exception:
            dcount = 0
        if dcount == 0:
            return "‚úÖ Aucun doublon d√©tect√© ‚Äî vos donn√©es semblent uniques."
        else:
            return f"‚ö†Ô∏è {dcount} doublons d√©tect√©s ‚Äî utilisez l'onglet 'Nettoyage' ‚Üí 'Supprimer les doublons'."
    
    # Questions sur les valeurs manquantes
    if "manquant" in question_lower or "missing" in question_lower or "null" in question_lower:
        if missing_total is not None:
            # Donner les 3 colonnes les plus touch√©es
            top_missing = []
            if missing_by_col is not None and len(missing_by_col) > 0:
                top = missing_by_col[missing_by_col > 0].head(3)
                for col, pct in top.items():
                    top_missing.append(f"{col}: {pct:.1f}%")
            top_text = ", ".join(top_missing) if top_missing else "Aucune valeur manquante significative d√©tect√©e"
            return f"üí° Total valeurs manquantes: {missing_total}. Colonnes les plus affect√©es: {top_text}."
        else:
            return "üí° Consultez l'onglet Analyse pour voir les valeurs manquantes par colonne."
    
    # Questions sur le nettoyage / next steps
    if "nettoyer" in question_lower or "clean" in question_lower or "probl√®me" in question_lower:
        # Fournir recommandations concr√®tes si on a df
        if df is not None:
            recs = []
            if doublons and int(doublons) > 0:
                recs.append(f"Supprimer {int(doublons)} doublons")
            if missing_total and missing_total > 0:
                recs.append(f"Traiter {missing_total} valeurs manquantes (ex: mean/median/ffill)")
            if numeric_cols:
                recs.append(f"V√©rifier outliers pour: {', '.join(numeric_cols[:3])}")
            if recs:
                return "üßπ Recommandations: " + "; ".join(recs)
            return "üßπ Pas d'action de nettoyage √©vidente ‚Äî v√©rifiez l'onglet Analyse pour d√©tails."
        return "üßπ Allez √† l'onglet **Nettoyage** pour supprimer les doublons, outliers et valeurs manquantes."
    
    # Questions sur l'analyse
    if "analyser" in question_lower or "analyse" in question_lower:
        if df is not None and len(numeric_cols) > 0:
            sample_stats = []
            for c in numeric_cols[:3]:
                mean = df[c].mean()
                median = df[c].median()
                sample_stats.append(f"{c}‚áí mean={mean:.2f}, median={median:.2f}")
            return "üìä Exemples de stats: " + "; ".join(sample_stats)
        return "üìä L'onglet Analyse affiche les stats : min, max, moyenne, m√©diane par colonne."
    
    # Questions sur la visualisation
    if "visuali" in question_lower or "graphique" in question_lower or "plot" in question_lower:
        return "üìà L'onglet Visualisation permet de cr√©er des graphiques interactifs (scatter, histogram, box, violin)."
    
    # Questions sur les pr√©dictions
    if "pr√©dict" in question_lower or "mod√®le" in question_lower or "predict" in question_lower:
        return "ü§ñ L'onglet Pr√©diction utilise un Random Forest pour classification/r√©gression et affiche score train/test."
    
    # Questions sur la qualit√©
    if "qualit√©" in question_lower or "quality" in question_lower:
        q = f"‚ú® Dataset: {colonnes} colonnes, {lignes} lignes."
        if missing_total is not None:
            q += f" Valeurs manquantes: {missing_total}."
        return q + " Qualit√©: voir onglet Analyse pour d√©tails."
    
    # R√©ponse par d√©faut
    return f"üí° Vous avez {lignes} lignes et {colonnes} colonnes. Explorez les onglets Analyse ‚Üí Nettoyage ‚Üí Visualisation pour plus de d√©tails." 


def extract_number(text: str, keyword: str) -> str:
    """Extraire un nombre du texte bas√© sur un mot-cl√©"""
    if not text:
        return "?"
    
    # Chercher le pattern "mot-cl√©: nombre"
    pattern = rf"{keyword}\s*:\s*(\d+)"
    match = re.search(pattern, text, re.IGNORECASE)
    
    if match:
        return match.group(1)
    
    return "?"


def extract_text_value(text: str, keyword: str) -> str:
    """Extraire une valeur du texte bas√©e sur un mot-cl√©"""
    if not text:
        return "N/A"
    
    # Chercher le pattern "mot-cl√©: valeur"
    pattern = rf"{keyword}\s*:\s*([^\n]+)"
    match = re.search(pattern, text, re.IGNORECASE)
    
    if match:
        return match.group(1).strip()
    
    return "N/A"

